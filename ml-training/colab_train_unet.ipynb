{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete U-Net Training Pipeline for Field Segmentation\n",
    "\n",
    "This comprehensive notebook guides you through the complete process of training a U-Net model for satellite imagery field segmentation in Google Colab, from initial setup to production deployment.\n",
    "\n",
    "## Overview\n",
    "- **Phase 1**: Environment Setup and Data Access\n",
    "- **Phase 2**: Configuration Setup\n",
    "- **Phase 3**: Training Execution\n",
    "- **Phase 4**: Evaluation\n",
    "- **Phase 5**: Export to Production\n",
    "\n",
    "## Prerequisites\n",
    "- Google Drive with Sentinel-2 dataset in `sentinel2_datasets/` folder\n",
    "- GitHub repository access (https://github.com/ns530/skycrop.git)\n",
    "- Colab runtime with GPU enabled (Runtime > Change runtime type > GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Environment Setup and Data Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"Google Drive mounted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Clone the SkyCrop repository\n",
    "!git clone https://github.com/ns530/skycrop.git\n",
    "%cd skycrop/ml-training\n",
    "print(\"Repository cloned and navigated to ml-training directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Install dependencies\n",
    "!pip install -r requirements.txt\n",
    "print(\"All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Set environment variables and verify setup\n",
    "import os\n",
    "\n",
    "# Set data and runs directories\n",
    "os.environ['DATA_DIR'] = '/content/drive/MyDrive'\n",
    "os.environ['RUNS_DIR'] = '/content/drive/MyDrive/runs'\n",
    "os.environ['MODEL_VERSION'] = '1.0.0'\n",
    "\n",
    "print(f\"DATA_DIR: {os.environ['DATA_DIR']}\")\n",
    "print(f\"RUNS_DIR: {os.environ['RUNS_DIR']}\")\n",
    "print(f\"MODEL_VERSION: {os.environ['MODEL_VERSION']}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 Verify dataset access\n",
    "dataset_path = '/content/drive/MyDrive/sentinel2_datasets'\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"‚úì Dataset found at: {dataset_path}\")\n",
    "    contents = os.listdir(dataset_path)\n",
    "    print(f\"Contents: {contents}\")\n",
    "    \n",
    "    # Check for expected subdirectories\n",
    "    expected_dirs = ['train', 'val', 'test']\n",
    "    for dir_name in expected_dirs:\n",
    "        dir_path = os.path.join(dataset_path, dir_name)\n",
    "        if os.path.exists(dir_path):\n",
    "            print(f\"‚úì {dir_name}/ directory exists\")\n",
    "        else:\n",
    "            print(f\"‚úó {dir_name}/ directory missing\")\n",
    "else:\n",
    "    print(f\"‚úó Dataset not found at: {dataset_path}\")\n",
    "    print(\"Please ensure your dataset is properly organized in Google Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.6 Detailed dataset structure verification\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_dataset_structure(base_path):\n",
    "    \"\"\"Analyze the complete dataset structure and file counts\"\"\"\n",
    "    structure = {}\n",
    "    total_files = 0\n",
    "    \n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        level = root.replace(base_path, '').count(os.sep)\n",
    "        indent = ' ' * 2 * level\n",
    "        structure[root] = {\n",
    "            'dirs': dirs,\n",
    "            'files': files,\n",
    "            'level': level\n",
    "        }\n",
    "        total_files += len(files)\n",
    "        \n",
    "        if level <= 2:  # Only show top levels\n",
    "            print(f\"{indent}üìÅ {os.path.basename(root)}/ ({len(files)} files)\")\n",
    "    \n",
    "    return structure, total_files\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    print(\"Dataset Structure Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    structure, total_files = analyze_dataset_structure(dataset_path)\n",
    "    print(f\"\\nTotal files in dataset: {total_files}\")\n",
    "    \n",
    "    # File type analysis\n",
    "    file_types = defaultdict(int)\n",
    "    for root, info in structure.items():\n",
    "        for file in info['files']:\n",
    "            ext = os.path.splitext(file)[1].lower()\n",
    "            file_types[ext] += 1\n",
    "    \n",
    "    print(\"\\nFile types:\")\n",
    "    for ext, count in sorted(file_types.items()):\n",
    "        print(f\"  {ext}: {count} files\")\n",
    "else:\n",
    "    print(\"Dataset path does not exist. Please check your Google Drive setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Load and examine the configuration file\n",
    "import yaml\n",
    "\n",
    "config_path = 'config.yaml'\n",
    "if os.path.exists(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    print(\"Current Configuration:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(yaml.dump(config, default_flow_style=False, indent=2))\n",
    "else:\n",
    "    print(f\"Configuration file not found at {config_path}\")\n",
    "    print(\"Please ensure you're in the correct directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Modify configuration for Colab environment\n",
    "import yaml\n",
    "\n",
    "# Load current config\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Update paths for Colab\n",
    "config['data']['data_dir'] = '/content/drive/MyDrive'\n",
    "config['data']['tiles_dir'] = '/content/drive/MyDrive/data/tiles'\n",
    "\n",
    "# Adjust training parameters for Colab\n",
    "config['train']['batch_size'] = 2  # Smaller batch size for Colab\n",
    "config['train']['epochs'] = 10     # Fewer epochs for testing\n",
    "\n",
    "# Save updated config\n",
    "with open('config_colab.yaml', 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, indent=2)\n",
    "\n",
    "print(\"Updated configuration saved as 'config_colab.yaml'\")\n",
    "print(\"Key changes:\")\n",
    "print(f\"- Data directory: {config['data']['data_dir']}\")\n",
    "print(f\"- Batch size: {config['train']['batch_size']}\")\n",
    "print(f\"- Epochs: {config['train']['epochs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Verify model registry\n",
    "import json\n",
    "\n",
    "registry_path = 'model_registry.json'\n",
    "if os.path.exists(registry_path):\n",
    "    with open(registry_path, 'r') as f:\n",
    "        registry = json.load(f)\n",
    "    \n",
    "    print(\"Current Model Registry:\")\n",
    "    print(\"=\" * 50)\n",
    "    if registry:\n",
    "        for model_name, versions in registry.items():\n",
    "            print(f\"Model: {model_name}\")\n",
    "            for version, info in versions.items():\n",
    "                print(f\"  Version {version}: {info.get('created_at', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"Registry is empty - no trained models yet\")\n",
    "else:\n",
    "    print(\"Model registry not found. Will be created during export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Run U-Net training\n",
    "print(\"Starting U-Net training...\")\n",
    "print(\"This may take several hours depending on your dataset size.\")\n",
    "print(\"Monitor progress in the cell output below.\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "!python train_unet.py --config config_colab.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Check training results\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Find the latest training run\n",
    "runs_dir = '/content/drive/MyDrive/runs'\n",
    "if os.path.exists(runs_dir):\n",
    "    runs = [d for d in os.listdir(runs_dir) if os.path.isdir(os.path.join(runs_dir, d))]\n",
    "    if runs:\n",
    "        latest_run = max(runs)\n",
    "        summary_path = os.path.join(runs_dir, latest_run, 'train_summary.json')\n",
    "        \n",
    "        if os.path.exists(summary_path):\n",
    "            with open(summary_path, 'r') as f:\n",
    "                summary = json.load(f)\n",
    "            \n",
    "            print(\"Training Summary:\")\n",
    "            print(\"=\" * 50)\n",
    "            print(f\"Status: {summary.get('status', 'Unknown')}\")\n",
    "            print(f\"Duration: {summary.get('elapsed_sec', 0)/3600:.2f} hours\")\n",
    "            print(f\"Epochs completed: {summary.get('epochs', 0)}\")\n",
    "            \n",
    "            if 'val_metrics' in summary:\n",
    "                metrics = summary['val_metrics']\n",
    "                print(f\"\\nValidation Metrics:\")\n",
    "                print(f\"  IoU: {metrics.get('iou', 0):.4f}\")\n",
    "                print(f\"  Dice: {metrics.get('dice', 0):.4f}\")\n",
    "                print(f\"  Loss: {metrics.get('loss', 0):.4f}\")\n",
    "            \n",
    "            if 'best_checkpoint' in summary:\n",
    "                print(f\"\\nBest checkpoint: {summary['best_checkpoint']}\")\n",
    "        else:\n",
    "            print(\"Training summary not found. Training may still be running or failed.\")\n",
    "    else:\n",
    "        print(\"No training runs found in runs directory.\")\n",
    "else:\n",
    "    print(\"Runs directory not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Monitor training with TensorBoard (optional)\n",
    "# Uncomment the following lines to start TensorBoard\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir /content/drive/MyDrive/runs\n",
    "print(\"To monitor training progress with TensorBoard:\")\n",
    "print(\"1. Uncomment the lines above\")\n",
    "print(\"2. Run this cell\")\n",
    "print(\"3. Click the TensorBoard link that appears\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Load and evaluate the trained model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def load_trained_model():\n",
    "    \"\"\"Load the best trained model from the latest run\"\"\"\n",
    "    runs_dir = '/content/drive/MyDrive/runs'\n",
    "    \n",
    "    if not os.path.exists(runs_dir):\n",
    "        print(\"Runs directory not found\")\n",
    "        return None\n",
    "    \n",
    "    runs = [d for d in os.listdir(runs_dir) if os.path.isdir(os.path.join(runs_dir, d))]\n",
    "    if not runs:\n",
    "        print(\"No training runs found\")\n",
    "        return None\n",
    "    \n",
    "    latest_run = max(runs)\n",
    "    summary_path = os.path.join(runs_dir, latest_run, 'train_summary.json')\n",
    "    \n",
    "    if not os.path.exists(summary_path):\n",
    "        print(\"Training summary not found\")\n",
    "        return None\n",
    "    \n",
    "    with open(summary_path, 'r') as f:\n",
    "        summary = json.load(f)\n",
    "    \n",
    "    checkpoint_path = summary.get('best_checkpoint')\n",
    "    if not checkpoint_path or not os.path.exists(checkpoint_path):\n",
    "        print(\"Best checkpoint not found\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Loading model from: {checkpoint_path}\")\n",
    "    model = tf.keras.models.load_model(checkpoint_path, compile=False)\n",
    "    return model\n",
    "\n",
    "model = load_trained_model()\n",
    "if model:\n",
    "    print(f\"‚úì Model loaded successfully\")\n",
    "    print(f\"  Input shape: {model.input_shape}\")\n",
    "    print(f\"  Output shape: {model.output_shape}\")\n",
    "else:\n",
    "    print(\"‚úó Failed to load model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Evaluate on test set\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, test_images_dir, test_masks_dir, num_samples=5):\n",
    "    \"\"\"Evaluate model on test set and visualize results\"\"\"\n",
    "    if not model:\n",
    "        print(\"No model available for evaluation\")\n",
    "        return\n",
    "    \n",
    "    # Get test files\n",
    "    image_files = sorted([f for f in os.listdir(test_images_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "    mask_files = sorted([f for f in os.listdir(test_masks_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "    \n",
    "    if len(image_files) != len(mask_files):\n",
    "        print(\"Mismatch between number of images and masks\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Evaluating on {len(image_files)} test samples\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    ious = []\n",
    "    dices = []\n",
    "    \n",
    "    for i, (img_file, mask_file) in enumerate(zip(image_files[:num_samples], mask_files[:num_samples])):\n",
    "        # Load and preprocess\n",
    "        img_path = os.path.join(test_images_dir, img_file)\n",
    "        mask_path = os.path.join(test_masks_dir, mask_file)\n",
    "        \n",
    "        img = np.array(Image.open(img_path).resize((512, 512))) / 255.0\n",
    "        mask = np.array(Image.open(mask_path).resize((512, 512))) / 255.0\n",
    "        mask = (mask > 0.5).astype(np.float32)\n",
    "        \n",
    "        # Predict\n",
    "        pred = model.predict(np.expand_dims(img, axis=0))[0]\n",
    "        pred_binary = (pred > 0.5).astype(np.float32)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        intersection = np.sum(pred_binary * mask)\n",
    "        union = np.sum(pred_binary) + np.sum(mask) - intersection\n",
    "        iou = intersection / union if union > 0 else 0\n",
    "        dice = 2 * intersection / (np.sum(pred_binary) + np.sum(mask)) if (np.sum(pred_binary) + np.sum(mask)) > 0 else 0\n",
    "        \n",
    "        ious.append(iou)\n",
    "        dices.append(dice)\n",
    "        \n",
    "        # Visualize first few samples\n",
    "        if i < 3:\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            axes[0].imshow(img)\n",
    "            axes[0].set_title('Input Image')\n",
    "            axes[1].imshow(mask, cmap='gray')\n",
    "            axes[1].set_title('Ground Truth')\n",
    "            axes[2].imshow(pred_binary, cmap='gray')\n",
    "            axes[2].set_title(f'Prediction\\nIoU: {iou:.3f}, Dice: {dice:.3f}')\n",
    "            plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Mean IoU: {np.mean(ious):.4f} ¬± {np.std(ious):.4f}\")\n",
    "    print(f\"Mean Dice: {np.mean(dices):.4f} ¬± {np.std(dices):.4f}\")\n",
    "    \n",
    "    return np.mean(ious), np.mean(dices)\n",
    "\n",
    "# Run evaluation\n",
    "test_images_dir = '/content/drive/MyDrive/sentinel2_datasets/test/test_images'\n",
    "test_masks_dir = '/content/drive/MyDrive/sentinel2_datasets/test/test_masks'\n",
    "\n",
    "if os.path.exists(test_images_dir) and os.path.exists(test_masks_dir):\n",
    "    mean_iou, mean_dice = evaluate_model(model, test_images_dir, test_masks_dir)\n",
    "else:\n",
    "    print(\"Test directories not found. Please check your dataset structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Run model tests\n",
    "print(\"Running model tests...\")\n",
    "!python -m pytest tests/test_unet_train_smoke.py -v\n",
    "print(\"Tests completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Export to Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Export the trained model\n",
    "print(\"Exporting trained model to production formats...\")\n",
    "!python export.py --config config_colab.yaml --version 1.0.0\n",
    "print(\"Export completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Verify exported model\n",
    "import os\n",
    "import json\n",
    "\n",
    "export_dir = '/content/skycrop/ml-training/u-net-trained-model/models/unet/1.0.0'\n",
    "if os.path.exists(export_dir):\n",
    "    print(f\"‚úì Export directory found: {export_dir}\")\n",
    "    contents = os.listdir(export_dir)\n",
    "    print(f\"Contents: {contents}\")\n",
    "    \n",
    "    # Check for required files\n",
    "    required_files = ['savedmodel.tar.gz', 'model.onnx', 'metrics.json', 'sha256.txt']\n",
    "    for file in required_files:\n",
    "        if file in contents:\n",
    "            print(f\"‚úì {file} present\")\n",
    "        else:\n",
    "            print(f\"‚úó {file} missing\")\n",
    "    \n",
    "    # Check metrics\n",
    "    metrics_path = os.path.join(export_dir, 'metrics.json')\n",
    "    if os.path.exists(metrics_path):\n",
    "        with open(metrics_path, 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        print(f\"\\nModel Metrics:\")\n",
    "        for key, value in metrics.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(f\"‚úó Export directory not found: {export_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Update and verify model registry\n",
    "import json\n",
    "\n",
    "registry_path = 'model_registry.json'\n",
    "if os.path.exists(registry_path):\n",
    "    with open(registry_path, 'r') as f:\n",
    "        registry = json.load(f)\n",
    "    \n",
    "    print(\"Updated Model Registry:\")\n",
    "    print(\"=\" * 50)\n",
    "    for model_name, versions in registry.items():\n",
    "        print(f\"Model: {model_name}\")\n",
    "        for version, info in versions.items():\n",
    "            print(f\"  Version {version}:\")\n",
    "            print(f\"    Created: {info.get('created_at', 'N/A')}\")\n",
    "            print(f\"    URI: {info.get('uri', 'N/A')}\")\n",
    "            if 'metrics' in info:\n",
    "                print(f\"    Metrics: {info['metrics']}\")\n",
    "else:\n",
    "    print(\"Model registry not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Test inference with exported model\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_onnx_inference(onnx_path, test_image_path):\n",
    "    \"\"\"Test inference with the exported ONNX model\"\"\"\n",
    "    if not os.path.exists(onnx_path):\n",
    "        print(f\"ONNX model not found: {onnx_path}\")\n",
    "        return\n",
    "    \n",
    "    # Load ONNX model\n",
    "    session = ort.InferenceSession(onnx_path)\n",
    "    \n",
    "    # Load and preprocess test image\n",
    "    if os.path.exists(test_image_path):\n",
    "        img = np.array(Image.open(test_image_path).resize((512, 512))) / 255.0\n",
    "        img = np.expand_dims(img, axis=0).astype(np.float32)\n",
    "        \n",
    "        # Run inference\n",
    "        outputs = session.run(None, {'input_1': img})\n",
    "        prediction = outputs[0][0]\n",
    "        \n",
    "        # Visualize\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img[0])\n",
    "        plt.title('Input Image')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(prediction > 0.5, cmap='gray')\n",
    "        plt.title('ONNX Prediction')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"‚úì ONNX inference test successful\")\n",
    "    else:\n",
    "        print(f\"Test image not found: {test_image_path}\")\n",
    "\n",
    "# Test with exported ONNX model\n",
    "onnx_path = 'models/unet/1.0.0/model.onnx'\n",
    "test_image_path = '/content/drive/MyDrive/sentinel2_datasets/test/test_images/' + os.listdir('/content/drive/MyDrive/sentinel2_datasets/test/test_images')[0]\n",
    "\n",
    "test_onnx_inference(onnx_path, test_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You have completed the complete U-Net training pipeline. Here's what you accomplished:\n",
    "\n",
    "### ‚úÖ Completed Phases:\n",
    "1. **Environment Setup**: Mounted Drive, cloned repo, installed dependencies\n",
    "2. **Configuration**: Modified config for Colab environment\n",
    "3. **Training**: Executed U-Net training with monitoring\n",
    "4. **Evaluation**: Assessed model performance on test set\n",
    "5. **Export**: Converted model to production formats (ONNX, SavedModel)\n",
    "\n",
    "### üìÅ Generated Artifacts:\n",
    "- Trained model checkpoints in `/content/drive/MyDrive/runs/`\n",
    "- Exported models in `models/unet/1.0.0/`\n",
    "- Updated model registry in `model_registry.json`\n",
    "- Training metrics and evaluation results\n",
    "\n",
    "### üöÄ Deployment Ready:\n",
    "Your trained U-Net model is now ready for deployment to the SkyCrop ML service for field segmentation tasks.\n",
    "\n",
    "### üîß Troubleshooting Tips:\n",
    "- **Memory Issues**: Reduce batch_size in config\n",
    "- **Long Training**: Use GPU runtime and monitor with TensorBoard\n",
    "- **Dataset Issues**: Verify file paths and formats\n",
    "- **Export Failures**: Check model loading and ONNX compatibility\n",
    "\n",
    "### üìö Additional Resources:\n",
    "- Check the SkyCrop documentation for ML service integration\n",
    "- Review the model registry for version management\n",
    "- Use the exported ONNX model for inference in production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
